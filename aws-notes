Highly available - System will continue to function despite a failure to any of the components in the underlying achitecture
Fault tolerant - Highly available with no degredation of performance.
Resilant - The power or ability of a material to return to its original form
Availability - Determined by the percentage of uptime (SLA in 9s) of a purticular resource.
Redundant - Mutiple resources performing same task
Scalability - increase the resources horizontally or vertically manual or automatic.
Elasticity - increase or decrease the resource based on load automatically
SLA can be found in - Well architected reliability whitepaper framework pages 54-58 / aws.amazon.com/<service>/sla
Route53 health check graph can give you your application's SLA


HA resilancy notes

Deploying a 3 tier Architecture for Highly available

Network ( VPC)

 AZ - Deploy the compute resources in atleast two AZ for resilancy
 Internet gateway - attached to vpc, highly available , redundent and horizontally scalbale
 VPC Peering connection - No underlying hardware, No bottleneck for scaling the traffic
 VPC End points - Similar to VPC Peering connection
 Virtual Private Gateway - Highly available but not redundent. There is two tunnels deployed in active passive mode in two different AZ.
                            At a time only one of them will be active. Route table should be configured to dynamically pull the routes incase of any failure with one tunnel. 
                            Resilancy will also depend on the complimentary connections and router ( dynamic or static) used in datacenter. The datacenter Customer Gateway should 
                            be configured with two tunnels.If there is a HW outage, it will cause short interuption of service.
 Nat Gateway - Highly available but blongs on one AZ. Best practice is to deploy different NAT Gateway for different Subnet and configure different routing tables for
               each subnet with corresponding gateway routes.There will be short interuption of service if underlying hardware of nat gateway goes off. AWS will replace it with new one.
               Note that Nat instance is deployed into EC2 and may not be a good idea when resilancy is a problem.
 
 
 Compute
 
 Elastic Load Balancers - Select eleastic load balancers to deploy into atleast two AZ.(99.99 Availabilty SLA)
 Auto Scaling - Like load balancing can deploy into multiple AZ. Service is  (99.99 Availabilty SLA) but indvidual compute SLA is 90% SLA. So stateful application will have inflight transaction 
                impact.
                
 Database
 RDS - RDS can be deployed into Multiple AZ but supports only single write end point only available in one zone. Multiple reads are supported. 99.95% uptime. This is where RDS Arrora comes in.
 
 RDS Arrora - Can be deployed into Multiple AZ , supports writes in all nodes while being consistant. Availlability is increased to 99.99% Uptime. 
            - Only support Mysql and Postgres with purticular versions.
            - Self healing, Auto scalable
  
 AWS Serverless Resilant Architecture
               
 DNS
 Route53 - This has 100% SLA with global scope. If it is down, Amazon will pay you.
 
 CDN
 CloudFront - This has 99.99% SLA with global scope.  
 
Storage:
 S3 - Bucket is regional but objects have to choosen to be anything other than one zone. 99.99% SLA

API Front end
API Gateway - Same as S3

Authentication and Authorization
Amazone Cognito - Regional , 99.99. Can be integrated with API Gateway for user authentication.

Compute functions
Lambda -Regional , 99.95% SLA

Database ( No SQL)
Dynamo DB - Regional, 99.99% SLA

Highly available / Fault tolarent section

Cache
ElasticCache - This is backed up by Ec2. Service is highly available but not fault tolerant or redundant.

Redshift - This is desined to be highly performance and is only in one AZ. So it is not HA.

RDS Multi AZ - If primary become degraded or patches, then AWS will failure to read only. But it is not fault tolarent. There will be a interuption in service.

S3 - Servlerless. All good.

DynamoDb - All good.

APIGateway - All Good.

CloudFront - All Good

Route53 - All good.


Zonal Resources section
 EBS Volume
 Nat Gateway
 Ec2
 Redshift Cluster
 RDS instances
 S3 one zone 1A
 Subnets
 
Regional scoped Resources ( Desined for HA and FT)
 All AWS Service end points ( IAM, EC2 service etc)
 S3 ( Except  one zone 1A)
 VPC
 CloudWatch
 Elastic load Balancers
 AutoScalingService
 
Multiple region supported resources.
S3 cross region replication
RDS cross region read replication
DynamoDb - Replication
AWS Edge location

AWS Global supported services
ClodFront53
Route53
WAF filtering rules
Lambda@Edge
Route53- > ClodFront53 - > WAF filtering rules - > Lambda@Edge

 
Decoupling components

Loadbalancers - For DR scenarios, deploy the components in two different regions with two different load balancers pointing to Autoscaling group. The router can be set up with different 
                options to support active active or passive set up. In case of both the regions are down, the router can also point to S3 bucket for a static message.
 
 Client-sns-sqs-autoscaling - This is mentioned but needs to be tried out on how SNS can be used to get client requests. SNS is a push based service , so it can probabliy 
 
Storage:
Instance volume storage - this is an antipattern for resilancy.
EBS - AZ scoped block storage. Kept redundent by replicating automatically to two different EBS in the same AZ. Availability : 4 9s. Durability .1-.2% failure.
      To make it rgional, use point in time snapshot and store it in S3. 
      If possible, do the snapshot back up to two different account so that even if one is gained by attacker, the data can be retreived from another account.
      
EFS - Region scope. Availability : 3 9s and Durability 11 9s. No published data for mount points and mount points are associated with subnets.AWS back up ( take the backup and store to AWS vault) 
                                   or AWS data sync can splicate the data into different region.
                                   
S3
 S3 standard - Region scope. 4 9s of availability and 11 9s of durabilty.
 S3 IA  - Region scope. 3 9s of availability and 11 9s durabilty.
 S3 Inteligent tier - This is a workload and not storage class. The numbers are similar to S3 IA.
 S3 One zone IA - Only one zone. Replicated to 3 times in same az. Durabiulity is same but avialability is 9.95
 
   To improve the durability, create another bucket in another region with versioning enabled. Then enable bucket versioning and cross region replications. 
    This only replicate the objects that  are created or updated after the replication is enabled. you still needs to copy the existing data.
    
 S3 Glacier - Region, numbers are identical to S3 standard. The latency can be from few minutes to few hours. 
 S3 Glacier Deep archieve - Same numbers but the latency is 12 hours

Performance :

Salabaility and Elasticity

EBS vloume size is scalable but not eleastic
unified AutoScaling template allows versioning and can be used to launch auto scaling groups, ECS, Arora read replica. T
hey are eleastic and even can be configured to be elastic based on a prediction with different options of k
eeping maximum scaling to hard limit or equal, above or within a buffer to predictable capacity. They can be also used to scale read/write dynamo DB.
Eleastic managed services for application examples - API gateway, ECS on Fargate, EC2 on Autoscaling, Dynamo DB with Autoscaling for database, Lamda for computing, S3 for storage


Database :

DB can be deployed in EC2 with EBS Volumes with a custom volume manager  - this is a high performant but low recillancy design and high management overhead.. 

Use RDS with read replica to another AZ. - This will take care of reads but not writes

Use Aurora - This will limit the choise to MYSQL and POSTGRES but will give multi master read and write. Using high configuration instance will incrase the performance.

Use Arora serverless - This will give an option for auto scaling and managing from aws.

Dynamo DB - Use srverless and partition key which is widly distributed for read enhanced performance. Use global table if applications is multi regional.

Dynamo DB Dax - Use this for caching to improve read performance.

aws well architecture framework

Enable:
 Lab :https://wellarchitectedlabs.com/

> By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud
> The AWS Well-Architected Tool (AWS WA Tool) is a service in the cloud that provides a consistent process for you to review and measure your architecture using the AWS Well-Architected Framework
> AWS Trusted Advisor is an online tool that provides you real time guidance to help you provision your resources following AWS best practices
> AWS also shares best practices and patterns that we have learned through the operation of AWS in The Amazon Builders' Library. A wide variety of other useful information is available through the AWS Blog and The Official AWS Podcast
> To help you apply best practices, we have created AWS Well-Architected Labs, which provides you with a repository of code and documentation to give you hands-on experience implementing best practices
> The AWS Well-Architected Framework is based on five pillars — operational excellence, security, reliability, performance efficiency, and cost optimization.
> Architecture is defining how components ( code, configuration, resources, etc) work together in a workload. Workload deliver business value.
> Technical portfolio is the collection of workload to catr business operations. 
> general design principles to facilitate good design in the cloud
   Stop guessing your capacity needs
   Test systems at production scale
   Automate to make architectural experimentation easier
   Allow for evolutionary architectures
   Drive architectures using data
   Improve through game days
   
 > Design principles for operational excellency
   Perform operations as code: 
   Make frequent, small, reversible changes
   Refine operations procedures frequently
   Anticipate failure
   Learn from all operational failures
   
   > Implementing services to enable integration,deployment, and delivery of your workload will enable an increased flow of beneficial changes into production by automating repetitive processes. 
    Evaluate external customer needs
    Evaluate internal customer needs
    Evaluate governance requirements
    Evaluate external compliance requirements - use AWS Cloud Compliance to help educate your teams so that they can determine the impact on your priorities.
    Evaluate threat landscape
    
    > Enterprise Support customers are eligible for a guided Well-Architected Review and Operational review
    
    > Advise in aws
    > The AWS Well-Architected Tool (AWS WA Tool) is a service in the cloud that provides a consistent process for you to review and measure your architecture using the AWS Well-Architected Framework
    > AWS Trusted Advisor is an online tool that provides you real time guidance to help you provision your resources following AWS best practices
    
    > Account management best practices 
     > Use AWS organization with well defined policies to manage different accounts in an organization. Billing can also be tied to organizational account.
     > Use AWS Control Tower features for automating well architect organization landing zone for accounts.
     
    > Resource management best pratices
     > AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS.
     > AWS config can scan though the infrastrucutre and keep an audit upto 7 years. This is integrated with organization.
    
    Videos:
    https://www.youtube.com/watch?v=bGBVPIpQMYk&feature=youtu.be
    https://www.youtube.com/watch?v=u8u9DXwNoIs&t= > Enable (Automating  multiple accounts users and infra)   + Provision (Add or deleted to the environment)  + Operation
    
    >  Operation with Agility tools offered by AWS
     
       Operation -  Monitor, Audit, Automate Operational actions,  continous improvment to optimize cost, architecture etc.
       
       Monitor : AWS cloudwatch
       Audit - AWS config ( infra config auditor) , AWS CloudTrail ( who did what?)
       Manage resources at scale - AWS System manager ( patch management)
       Optimize cost and resources : AWS Trusted advisor, AWS Cost explorer, AWS cost and usage report
       
   AWS Training resources:
   AWS provides resources, including the AWS Getting Started Resource Center, AWS Blogs, AWS Online Tech Talks, AWS Events and Webinars, and the AWS WellArchitected Labs, that provide guidance, examples, and detailed walkthroughs to
   educate your teams.AWS also shares best practices and patterns that we have learned through the operation of AWS in The Amazon Builders' Library and a wide variety of other useful
   educational material through the AWS Blog and The Official AWS Podcast.You should take advantage of the education resources provided by AWS such as the Well-Architected labs, AWS Support (AWS Knowledge Center, AWS Discussion Forms,
   and AWS Support Center) and AWS Documentation to educate your teams. Reach out to AWS Support through AWS Support Center for help with your AWS questions. AWS Training and Certification provides some free training through self-paced digital
   courses on AWS fundamentals. You can also register for instructor-led training to further support the development of your teams’ AWS skills.
   

Prepare:
 Design Telemetry:
   Application telemetry: Instrument your application code to emit information about its internal state, status, and achievement of business outcomes, for example, queue depth, error messages,
      and response times. Use this information to determine when a response is requd.
   
      Unified Amazon CloudWatch Logs Agent - Use it to capture system logs and advanced metrics from EC2 instances.
      Logs
        Application -> directly -> cloudWatch logs API
        AWS Lamda -> sysout automatic- > Cloudwatch logs
      Events
        Application->AWS SDK -> Amazone event Bridge
        
  Workload telemetry:
     : Design and configure your workload to emit information about its internal state and current status. For example, API call volume, HTTP status codes,and scaling events. Use this information to help determine when a response is required.
     AWS CloudTrail -> AWS Cloudwatch
     VPC Flow logs -> AWS Cloudwatch
     AWS Lambda metrics -> AWS Cloudwatch
   
   Implement user activity telemetry:
      :Instrument your application code to emit information about user activity, for example, click streams, or started, abandoned, and completed transactions. Use this
      information to help understand how the application is used, patterns of usage, and to determine when a response is required.
      
   Implement dependency telemetry: 
     Design and configure your workload to emit information about the status (for example, reachability or response time) of resources it depends on. Examples of external
     dependencies can include external databases, DNS, and network connectivity. Use this information to determine when a response is required.

  Implement transaction traceability: Implement your application code and configure your workload components to emit information about the flow of transactions across the workload. Use this
    information to determine when a response is required and to assist you in identifying the factors contributing to an issue.
    
      AWS X-Ray: to collect and record traces as transactions travel through your workload

CI/CD automation
  Image : AMI ( Linux)  , EC2 image builder( Windows)
  Code commit : AWS code commit , or git hub etc, S3
  Build -Jenkins / any third party / AWS CloudBuild
  Artifact store : AWS codeArtifacts
  Amazon Elastic Container Registry: Store images
  Test - Any Thirdparty 
  Deploy :
    Code:
     AWS code deploy - AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises    
     AWS Elastic bean stack - Fully managed service to Deploy the code . provision of infrastructure ( like load balancers) will be taken care by aws. Developers will still have control on the infra created.
    Infra:
     AWS Cloud formation - IAAC
     AWS OpsWorks - Managed chef and pupput instance for automation
  Notification - SNS
  Invoke logic - Lambda
  AWS CodePipeline - Define SDLC workflow of the code CI/CD pipeline by combining all the above steps.It is better to have seperate pipeline for application code and infra strucutre code. Infrastructure code, even can be seperated into networking and others if needed.
  AWS cloudformation - Infrastructure as code templates. 

   

Design for Operations:
 > workload : applications, infrastructure, policy, governance, and operations - Treat this as code
 > Use version control: AWS code commit
 > Use IAAC : AWS Cloudformation or OpsWorks and BeanStack 
 > Test and validate changes: Create temporary parallel environment
 > Use build and deployment management systems : Use https://aws.amazon.com/products/developer-tools/
 > Perform patch management: AWS Systems Manager Patch Manager ( Automate), AWS Systems Manager maintanance windows (Schedule patch) , AWS 
 > Share design standards:> Many AWS services and resources are designed to be shared across accounts, enabling you to share created assets and learnings across your teams. For example, you can share CodeCommit repositories,
   Lambda functions, Amazon S3 buckets, and AMIs to specific accounts.
 > When you publish new resources or updates, use Amazon SNS to provide cross account notification .Subscribers can use Lambda to get new versions
> Implement practices to improve code quality: Implement practices to improve code quality and minimize defects. For example, test-driven development, code reviews, and standards adoption.
> Use multiple environments: Use multiple environments to experiment, develop, and test your workload.Use increasing levels of controls as environments approach production to gain confidence your workload
 will operate as intended when deployed.
 >Make frequent, small, reversible changes:
 >Apply metadata using Resource Tags and AWS Resource Groups following a consistent tagging strategy (https://d1.awsstatic.com/whitepapers/aws-tagging-best-practices.pdf)
 

Mitigate Deployment Risks 
 > Plan for unsuccessful changes
 > Test and validate changes:
 > Use deployment management systems
 > Test using limited deployment / Deploy using parallel environments:
 > Deploy frequent, small, reversible changes:
 > Fully automate integration and deployment
 > Automate testing and rollback:
 
Operational Readiness
 > You should use a consistent process (including manual or automated checklists) to know when you are ready to go live with your workload or a change. This will also enable you to find any areas that you need
  to make plans to address. You will have runbooks that document your routine activities and playbooks that guide your processes for issue resolution.
 > Ensure personnel capability:
 >Ensure consistent review of operational readiness
  > AWS Config : You should automate workload configuration testing by making baselines
  > AWS Config rules : checking your configurations with baseline
  >  AWS Security Hub: You can evaluate security requirements and compliance using the services and features of security hub.AWS Security Hub gives you a comprehensive view of your security alerts and security posture across your AWS accounts.
 >Use runbooks to perform procedures:
 >Use playbooks to identify issues:
 >Automate run book and play book
   AWS System manager( automate Run books or play book) 
   Lambda - respond to events
   AWS Step Functions - Orchestrate different aws services
   Trigger : AWS Cloudwatch evets
   AWS EventBridge : Support systems
  >Do pre mortem
  
  
  Operate
     
    

 
 










