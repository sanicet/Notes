General:

IOPS measures the number of read and write operations per second, while throughput measures the number of bits read or written per second
EBS is a block storgae and is not optimized for using it as file share
EFS is based on NFS / SMB and is used as a file share mount point

Subscribe : aws this week

Highly available - System will continue to function despite a failure to any of the components in the underlying achitecture
Fault tolerant - Highly available with no degredation of performance.
Resilant - The power or ability of a material to return to its original form
Availability - Determined by the percentage of uptime (SLA in 9s) of a purticular resource.
Redundant - Mutiple resources performing same task
Scalability - increase the resources horizontally or vertically manual or automatic.
Elasticity - increase or decrease the resource based on load automatically
SLA can be found in - Well architected reliability whitepaper framework pages 54-58 / aws.amazon.com/<service>/sla
Route53 health check graph can give you your application's SLA


HA resilancy notes

Deploying a 3 tier Architecture for Highly available

Network ( VPC)

 AZ - Deploy the compute resources in atleast two AZ for resilancy
 Internet gateway - attached to vpc, highly available , redundent and horizontally scalbale
 VPC Peering connection - No underlying hardware, No bottleneck for scaling the traffic
 VPC End points - Similar to VPC Peering connection
 Virtual Private Gateway - Highly available but not redundent. There is two tunnels deployed in active passive mode in two different AZ.
                            At a time only one of them will be active. Route table should be configured to dynamically pull the routes incase of any failure with one tunnel. 
                            Resilancy will also depend on the complimentary connections and router ( dynamic or static) used in datacenter. The datacenter Customer Gateway should 
                            be configured with two tunnels.If there is a HW outage, it will cause short interuption of service.
 Nat Gateway - Highly available but blongs on one AZ. Best practice is to deploy different NAT Gateway for different Subnet and configure different routing tables for
               each subnet with corresponding gateway routes.There will be short interuption of service if underlying hardware of nat gateway goes off. AWS will replace it with new one.
               Note that Nat instance is deployed into EC2 and may not be a good idea when resilancy is a problem.
 
 
 Compute
 
 Elastic Load Balancers - Select eleastic load balancers to deploy into atleast two AZ.(99.99 Availabilty SLA)
 Auto Scaling - Like load balancing can deploy into multiple AZ. Service is  (99.99 Availabilty SLA) but indvidual compute SLA is 90% SLA. So stateful application will have inflight transaction 
                impact.
                
 CAAS - ECS, EKS, Fargate(Serverless ECS/EKS mode)
                
 Database
 RDS - RDS can be deployed into Multiple AZ but supports only single write end point only available in one zone. Multiple reads are supported. 99.95% uptime. This is where RDS Arrora comes in.
 
 RDS Arrora - Can be deployed into Multiple AZ , supports writes in all nodes while being consistant. Availlability is increased to 99.99% Uptime. 
            - Only support Mysql and Postgres with purticular versions.
            - Self healing, Auto scalable
  
 AWS Serverless Resilant Architecture
               
 DNS
 Route53 - This has 100% SLA with global scope. If it is down, Amazon will pay you.
 
 CDN
 CloudFront - This has 99.99% SLA with global scope.  
 
Storage:
 S3 - Bucket is regional but objects have to choosen to be anything other than one zone. 99.99% SLA

API Front end
API Gateway - Same as S3

Authentication and Authorization
Amazone Cognito - Regional , 99.99. Can be integrated with API Gateway for user authentication.

Compute functions
Lambda -Regional , 99.95% SLA

Database ( No SQL)
Dynamo DB - Regional, 99.99% SLA

Highly available / Fault tolarent section

Cache
ElasticCache - This is backed up by Ec2. Service is highly available but not fault tolerant or redundant.

Redshift - This is desined to be highly performance and is only in one AZ. So it is not HA.

RDS Multi AZ - If primary become degraded or patches, then AWS will failure to read only. But it is not fault tolarent. There will be a interuption in service.

S3 - Servlerless. All good.

DynamoDb - All good.

APIGateway - All Good.

CloudFront - All Good

Route53 - All good.


Zonal Resources section
 EBS Volume
 Nat Gateway
 Ec2
 Redshift Cluster
 RDS instances
 S3 one zone 1A
 Subnets
 
Regional scoped Resources ( Desined for HA and FT)
 All AWS Service end points ( IAM, EC2 service etc)
 S3 ( Except  one zone 1A)
 VPC
 CloudWatch
 Elastic load Balancers
 AutoScalingService
 
Multiple region supported resources.
S3 cross region replication
RDS cross region read replication
DynamoDb - Replication
AWS Edge location

AWS Global supported services
ClodFront53
Route53
WAF filtering rules
Lambda@Edge
Route53- > ClodFront53 - > WAF filtering rules - > Lambda@Edge

 
Decoupling components

Loadbalancers - For DR scenarios, deploy the components in two different regions with two different load balancers pointing to Autoscaling group. The router can be set up with different 
                options to support active active or passive set up. In case of both the regions are down, the router can also point to S3 bucket for a static message.
 
 Client-sns-sqs-autoscaling - This is mentioned but needs to be tried out on how SNS can be used to get client requests. SNS is a push based service , so it can probabliy 
 
Storage:
Instance volume storage - this is an antipattern for resilancy.
EBS - AZ scoped block storage. Kept redundent by replicating automatically to two different EBS in the same AZ. Availability : 4 9s. Durability .1-.2% failure.
      To make it rgional, use point in time snapshot and store it in S3. 
      If possible, do the snapshot back up to two different account so that even if one is gained by attacker, the data can be retreived from another account.
      
EFS - Region scope. Availability : 3 9s and Durability 11 9s. No published data for mount points and mount points are associated with subnets.AWS back up ( take the backup and store to AWS vault) 
                                   or AWS data sync can splicate the data into different region.
                                   
S3
 S3 standard - Region scope. 4 9s of availability and 11 9s of durabilty.
 S3 IA  - Region scope. 3 9s of availability and 11 9s durabilty.
 S3 Inteligent tier - This is a workload and not storage class. The numbers are similar to S3 IA.
 S3 One zone IA - Only one zone. Replicated to 3 times in same az. Durabiulity is same but avialability is 9.95
 
   To improve the durability, create another bucket in another region with versioning enabled. Then enable bucket versioning and cross region replications. 
    This only replicate the objects that  are created or updated after the replication is enabled. you still needs to copy the existing data.
    
 S3 Glacier - Region, numbers are identical to S3 standard. The latency can be from few minutes to few hours. 
 S3 Glacier Deep archieve - Same numbers but the latency is 12 hours

Performance :

Salabaility and Elasticity

EBS vloume size is scalable but not eleastic
unified AutoScaling template allows versioning and can be used to launch auto scaling groups, ECS, Arora read replica. T
hey are eleastic and even can be configured to be elastic based on a prediction with different options of k
eeping maximum scaling to hard limit or equal, above or within a buffer to predictable capacity. They can be also used to scale read/write dynamo DB.
Eleastic managed services for application examples - API gateway, ECS on Fargate, EC2 on Autoscaling, Dynamo DB with Autoscaling for database, Lamda for computing, S3 for storage


Database :

DB can be deployed in EC2 with EBS Volumes with a custom volume manager  - this is a high performant but low recillancy design and high management overhead.. 

Use RDS with read replica to another AZ. - This will take care of reads but not writes

Use Aurora - This will limit the choise to MYSQL and POSTGRES but will give multi master read and write. Using high configuration instance will incrase the performance.

Use Arora serverless - This will give an option for auto scaling and managing from aws.

Dynamo DB - Use srverless and partition key which is widly distributed for read enhanced performance. Use global table if applications is multi regional.

Dynamo DB Dax - Use this for caching to improve read performance.

aws well architecture framework

Enable:
 Lab :https://wellarchitectedlabs.com/

> By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud
> The AWS Well-Architected Tool (AWS WA Tool) is a service in the cloud that provides a consistent process for you to review and measure your architecture using the AWS Well-Architected Framework
> AWS Trusted Advisor is an online tool that provides you real time guidance to help you provision your resources following AWS best practices
> AWS also shares best practices and patterns that we have learned through the operation of AWS in The Amazon Builders' Library. A wide variety of other useful information is available through the AWS Blog and The Official AWS Podcast
> To help you apply best practices, we have created AWS Well-Architected Labs, which provides you with a repository of code and documentation to give you hands-on experience implementing best practices
> The AWS Well-Architected Framework is based on five pillars — operational excellence, security, reliability, performance efficiency, and cost optimization.
> Architecture is defining how components ( code, configuration, resources, etc) work together in a workload. Workload deliver business value.
> Technical portfolio is the collection of workload to catr business operations. 
> general design principles to facilitate good design in the cloud
   Stop guessing your capacity needs
   Test systems at production scale
   Automate to make architectural experimentation easier
   Allow for evolutionary architectures
   Drive architectures using data
   Improve through game days
   
 > Design principles for operational excellency
   Perform operations as code: 
   Make frequent, small, reversible changes
   Refine operations procedures frequently
   Anticipate failure
   Learn from all operational failures
   
   > Implementing services to enable integration,deployment, and delivery of your workload will enable an increased flow of beneficial changes into production by automating repetitive processes. 
    Evaluate external customer needs
    Evaluate internal customer needs
    Evaluate governance requirements
    Evaluate external compliance requirements - use AWS Cloud Compliance to help educate your teams so that they can determine the impact on your priorities.
    Evaluate threat landscape
    
    > Enterprise Support customers are eligible for a guided Well-Architected Review and Operational review
    
    > Advise in aws
    > The AWS Well-Architected Tool (AWS WA Tool) is a service in the cloud that provides a consistent process for you to review and measure your architecture using the AWS Well-Architected Framework
    > AWS Trusted Advisor is an online tool that provides you real time guidance to help you provision your resources following AWS best practices
    
    > Account management best practices 
     > Use AWS organization with well defined policies to manage different accounts in an organization. Billing can also be tied to organizational account.
     > Use AWS Control Tower features for automating well architect organization landing zone for accounts.
     
    > Resource management best pratices
     > AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS.
     > AWS config can scan though the infrastrucutre and keep an audit upto 7 years. This is integrated with organization.
    
    Videos:
    https://www.youtube.com/watch?v=bGBVPIpQMYk&feature=youtu.be
    https://www.youtube.com/watch?v=u8u9DXwNoIs&t= > Enable (Automating  multiple accounts users and infra)   + Provision (Add or deleted to the environment)  + Operation
    
    >  Operation with Agility tools offered by AWS
     
       Operation -  Monitor, Audit, Automate Operational actions,  continous improvment to optimize cost, architecture etc.
       
       Monitor : AWS cloudwatch
       Audit - AWS config ( infra config auditor) , AWS CloudTrail ( who did what?)
       Manage resources at scale - AWS System manager ( patch management)
       Optimize cost and resources : AWS Trusted advisor, AWS Cost explorer, AWS cost and usage report
       
   AWS Training resources:
   AWS provides resources, including the AWS Getting Started Resource Center, AWS Blogs, AWS Online Tech Talks, AWS Events and Webinars, and the AWS WellArchitected Labs, that provide guidance, examples, and detailed walkthroughs to
   educate your teams.AWS also shares best practices and patterns that we have learned through the operation of AWS in The Amazon Builders' Library and a wide variety of other useful
   educational material through the AWS Blog and The Official AWS Podcast.You should take advantage of the education resources provided by AWS such as the Well-Architected labs, AWS Support (AWS Knowledge Center, AWS Discussion Forms,
   and AWS Support Center) and AWS Documentation to educate your teams. Reach out to AWS Support through AWS Support Center for help with your AWS questions. AWS Training and Certification provides some free training through self-paced digital
   courses on AWS fundamentals. You can also register for instructor-led training to further support the development of your teams’ AWS skills.
   

Prepare:
 Design Telemetry:
   Application telemetry: Instrument your application code to emit information about its internal state, status, and achievement of business outcomes, for example, queue depth, error messages,
      and response times. Use this information to determine when a response is requd.
   
      Unified Amazon CloudWatch Logs Agent - Use it to capture system logs and advanced metrics from EC2 instances.
      Logs
        Application -> directly -> cloudWatch logs API
        AWS Lamda -> sysout automatic- > Cloudwatch logs
      Events
        Application->AWS SDK -> Amazone event Bridge
        
  Workload telemetry:
     : Design and configure your workload to emit information about its internal state and current status. For example, API call volume, HTTP status codes,and scaling events. Use this information to help determine when a response is required.
     AWS CloudTrail -> AWS Cloudwatch
     VPC Flow logs -> AWS Cloudwatch
     AWS Lambda metrics -> AWS Cloudwatch
   
   Implement user activity telemetry:
      :Instrument your application code to emit information about user activity, for example, click streams, or started, abandoned, and completed transactions. Use this
      information to help understand how the application is used, patterns of usage, and to determine when a response is required.
      
   Implement dependency telemetry: 
     Design and configure your workload to emit information about the status (for example, reachability or response time) of resources it depends on. Examples of external
     dependencies can include external databases, DNS, and network connectivity. Use this information to determine when a response is required.

  Implement transaction traceability: Implement your application code and configure your workload components to emit information about the flow of transactions across the workload. Use this
    information to determine when a response is required and to assist you in identifying the factors contributing to an issue.
    
      AWS X-Ray: to collect and record traces as transactions travel through your workload

CI/CD automation
  Image : AMI ( Linux)  , EC2 image builder( Windows)
  Code commit : AWS code commit , or git hub etc, S3
  Build -Jenkins / any third party / AWS CloudBuild
  Artifact store : AWS codeArtifacts
  Amazon Elastic Container Registry: Store images
  Test - Any Thirdparty 
  Deploy :
    Code:
     AWS code deploy - AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises    
     AWS Elastic bean stack - Fully managed service to Deploy the code . provision of infrastructure ( like load balancers) will be taken care by aws. Developers will still have control on the infra created.
    Infra:
     AWS Cloud formation - IAAC
     AWS OpsWorks - Managed chef and pupput instance for automation
  Notification - SNS
  Invoke logic - Lambda
  AWS CodePipeline - Define SDLC workflow of the code CI/CD pipeline by combining all the above steps.It is better to have seperate pipeline for application code and infra strucutre code. Infrastructure code, even can be seperated into networking and others if needed.
  AWS cloudformation, Serverless application model (SAM) - Infrastructure as code templates. 

   

Design for Operations:
 > workload : applications, infrastructure, policy, governance, and operations - Treat this as code
 > Use version control: AWS code commit
 > Use IAAC : AWS Cloudformation Serverless application model (SAM) or OpsWorks and BeanStack
 > Test and validate changes: Create temporary parallel environment
 > Use build and deployment management systems : Use https://aws.amazon.com/products/developer-tools/
 > Perform patch management: AWS Systems Manager Patch Manager ( Automate), AWS Systems Manager maintanance windows (Schedule patch) , AWS 
 > Share design standards:> Many AWS services and resources are designed to be shared across accounts, enabling you to share created assets and learnings across your teams. For example, you can share CodeCommit repositories,
   Lambda functions, Amazon S3 buckets, and AMIs to specific accounts.
 > When you publish new resources or updates, use Amazon SNS to provide cross account notification .Subscribers can use Lambda to get new versions
> Implement practices to improve code quality: Implement practices to improve code quality and minimize defects. For example, test-driven development, code reviews, and standards adoption.
> Use multiple environments: Use multiple environments to experiment, develop, and test your workload.Use increasing levels of controls as environments approach production to gain confidence your workload
 will operate as intended when deployed.
 >Make frequent, small, reversible changes:
 >Apply metadata using Resource Tags and AWS Resource Groups following a consistent tagging strategy (https://d1.awsstatic.com/whitepapers/aws-tagging-best-practices.pdf)
 

Mitigate Deployment Risks 
 > Plan for unsuccessful changes
 > Test and validate changes:
 > Use deployment management systems
 > Test using limited deployment / Deploy using parallel environments:
 > Deploy frequent, small, reversible changes:
 > Fully automate integration and deployment
 > Automate testing and rollback:
 
Operational Readiness
 > You should use a consistent process (including manual or automated checklists) to know when you are ready to go live with your workload or a change. This will also enable you to find any areas that you need
  to make plans to address. You will have runbooks that document your routine activities and playbooks that guide your processes for issue resolution.
 > Ensure personnel capability:
 >Ensure consistent review of operational readiness
  > AWS Config : You should automate workload configuration testing by making baselines
  > AWS Config rules : checking your configurations with baseline
  >  AWS Security Hub: You can evaluate security requirements and compliance using the services and features of security hub.AWS Security Hub gives you a comprehensive view of your security alerts and security posture across your AWS accounts.
 >Use runbooks to perform procedures:
 >Use playbooks to identify issues:
 >Automate run book and play book
   AWS System manager( automate Run books or play book) 
   Lambda - respond to events
   AWS Step Functions - Orchestrate different aws services
   Trigger : AWS Cloudwatch evets
   AWS EventBridge : Support systems
  >Do pre mortem
  
  
  Operate
     
    > Identify key performance indicators: Identify key performance indicators (KPIs) based on desired business outcomes (for example, order rate, customer retention rate, and profit versus operating expense) and customer outcomes (for example, customer
    satisfaction). Evaluate KPIs to determine workload success.
    
    > Define workload metrics: Define workload metrics to measure the achievement of KPIs (for example, abandoned shopping carts, orders placed, cost, price, and allocated workload expense). Define workload metrics to measure the health of the workload (for
     example, interface response time, error rate, requests made, requests completed, and utilization). Evaluate metrics to determine if the workload is achieving desired outcomes, and to understand the health of the workload.
    
    >Collect and analyze workload metrics:
      AWS cloud watch - use it as a log agrregater and create metricks from it and measure it against KPis
      AWS Personal Health Dashboard > . This dashboard provides alerts and remediation guidance when AWS is experiencing events that might affect you.
      AWS Health API > Customers with Business and Enterprise Support subscriptions also get access to the , enabling integration to their event management systems.

     Logs
      Workload, Audit -> cloudwatch -> export -> S3 -> discover and prepare data (AWS Glue) to data catelog -> Amazon Athena ( ANalyse using SQL) -> Amazon quick sight ( visualization tool)
      alternate solution : Workload -> ELK/EFK
      
        AWS Glue is a fully managed data catalog and ETL (extract, transform, and load) service that simplifies and automates the difficult and time-consuming tasks of data discovery, conversion, and job scheduling. AWS Glue crawls your data sources and constructs a data catalog using pre-built classifiers for popular data formats and data types, including CSV, Apache Parquet, JSON, and more.
        Because AWS Glue is integrated with Amazon S3, Amazon RDS, Amazon Athena, Amazon Redshift, and Amazon Redshift Spectrum—the core components of a modern data architecture—it works seamlessly to orchestrate the movement and management of your data.
        
        Amazon Atehena - Analysis using SQL.
        
        Amazon QuickSight -  you can visualize, explore, and analyze your data.
        
        in AWS, you can improve recovery time by replacing failed components with known good versions, rather than trying to repair them
       
        On AWS, you can use AWS Systems Manager OpsCenter as a central location to view, investigate, and resolve operational issues related to any AWS resource. It aggregates
       operational issues and provides contextually relevant data to assist in incident response.
       
       You should use CloudTrail to track API activity (through the AWS Management Console, CLI, SDKs, and APIs) to know what is happening across your accounts
       
       Track your AWS Developer Tools deployment activities with CloudTrail and CloudWatch. This will add a detailed activity history of your deployments and their outcomes to your
       CloudWatch Logs log data.
       
       
Cost Optimization Pillar
 
       Adopt a consumption model: 
        > Pay only for the computing resources you consume,and increase or decrease usage depending on business requirements. For example,development and test environments are typically only used for eight hours a day during
        the work week

       Measure overall efficiency
       Stop spending money on undifferentiated heavy lifting:
       Analyze and attribute expenditure
      
      focus areas for cost optimization in the cloud:
      
       You can use AWS Cost Explorer to forecast daily (up to 3 months) or monthly (up to 12 months) cloud costs based on machine learning algorithms applied to your historical
       costs (trend based).
       
       AWS Cost Explorer provides dashboards and reports. You can track your progress of cost and usage against configured budgets with AWS Budgets Reports. You can also use Amazon QuickSight with Cost and Usage Report (CUR) data, to
       provide highly customized reporting with more granular daata
       
       You may be able to implement new AWS services and features to increase cost efficiency in your workload. Regularly review the AWS News Blog, the AWS Cost Management blog, and What’s New with AWS for
       information on new service and feature releases.
       
       Within AWS Organizations, consolidated billing creates the construct between one or more member accounts and the master account. Member accounts allow you to isolate 
       Amazon Web Services Cost Optimization Pillar 12and distinguish your cost and usage by groups. A common practice is to have separatemember accounts for each organization unit (such as finance, marketing, and sales), or
       for each environment lifecycle (such as development, testing and production), or foreach workload (workload a, b, and c), and then aggregate these linked accounts usingonsolidated billing. 
        
        
       As costs and usage are aggregated in the master account, this allows you to maximize your service volume discounts, and maximize the use of your commitment discounts (Savings Plans and Reserved Instances) to achieve the highest
       discounts.
       
       In AWS, notifications are conducted with AWS Budgets, which allows you to define a monthly budget for your AWS costs, usage, and commitment discounts (Savings Plans and Reserved Instances).

       You can use AWS Auto Scaling to perform the decommissioning process. You can also implement custom code using the API or SDK to decommission workload resources automatically

        You can also use the AWS Simple Monthly Calculator or the AWS Pricing Calculator to estimate workload costs

        You can use AWS License Manager to manage the software licenses in your workload. 
        
        AWS Compute Optimizer can assist with cost modeling for running workloads. It provides right-sizing recommendations for compute resources based on historical  Amazon Web Services Cost Optimization Pillar 22 usage.
        

       Select the Best Pricing Model
       AWS has multiple pricing models that allow you to pay for your resources in the most cost-effective way that suits your organization’s needs. The following section describes each purchasing model

            .On-Demand
              OnDemand has an hourly rate, but depending on the service, can be billed in increments of 1 second (for example Amazon RDS, or Linux EC2 instances)
              On demand is recommended for applications with short-term workloads (for example, a four-month project), that spike periodically, or unpredictable workloads that can’t be interrupted. On
              demand is also suitable for workloads, such as pre-production environments, which require uninterrupted runtimes, but do not run long enough for a commitment discount 
            
            •Spot
              A Spot Instance is spare EC2 compute capacity available at discounts of up to 90% off On-Demand prices with no long-term commitment required
              2-minute warning if EC2 needs the capacity back  or the Spot Instance price exceeds your configured price
              On average, Spot Instances are interrupted less than 5% of the time
              Spot is ideal when the workload is non critical or has a queue in place and there is multiple insantces processing a workload with stateless and fault tolerant application like hadoop.
              Non-critical workloads such as test and development environments are also candidates for Spot
              Spot is integrated with autoscaling group, elasitic container service, elastic map reduce, AWS batch
              When two minutes is over, AWS sends a interruption notice through cloud watch event and EC2 metadata service. Use the two minutes to save any final workload.
              Spot prices are hourly and if it goes up that your choice, you have two minutes to decide to stop or terminate the instances. You can also choose too have
              Spot blocks which will block the termination upto 6 hours of running time after a spot price increase beyond the threshold.Use spot pricing history to decide on the price.
              Spot request ith one time option will not relaunch the instances even if spot price goes below the threashold. But with persitance, it relaunches.
              The spot request will be in active status when instances are running , it will be in disabled status when it is in persistance mode and when it is not running. Or it will
              be in closed/failed/cancelled mode depending on the action that was done.
              Best practices:  
               Set your maximum price as the On-Demand rate
               Be flexible across as many instance types as possible
               Be flexible about where(AZ) your workload will run
               Design your workloads for statelessness and faulttolerance
               We recommend using Spot Instances in combination with On-Demand and Savings Plans/Reserved Instances to maximize workload cost optimization with performance
               
            • Commitment discounts - Savings Plans
               A Savings Plan allows you to make an hourly spend commitment for one or three years, and receive discounted pricing across your resources.
               When you make the commitment, you pay that commitment amount every hour, and it is subtracted from your On-Demand usage at the discount rate
               Compute Savings Plans are the most flexible and provide a discount of up to 66%. They automatically apply across Availability Zones, instance size, instance family, operating
               system, tenancy, Region, and compute service. Region is the point here compared to instance discount.
              
            • Commitment discounts - Reserved Instances/Capacity
              Instance Savings Plans have less flexibility but provide a higher discount rate (up to 72%). They automatically apply across Availability Zones, instance size, instance family,
              operating system, and tenancy
              There are three payment options:
               • No upfront payment:
               • Partial upfront payment
               • All upfront payment         
                           
            • Geographic selection
              Each AWS Region operates within local market conditions, and resource pricing is different in each Region. Choose a specific Region to operate a component of or your
              entire solution so that you can run at the lowest possible price globally
            
            • EC2 Fleet
              EC2 Fleet is a feature that allows you to define a target compute capacity,and then specify the instance types and the balance of On-Demand and Spot for the
              fleet. EC2 Fleet will automatically launch the lowest price combination of resources to meet the defined capacity.
            
            
            Best Practise for savings plan:
            Savings plans apply first to the usage in the account they are purchased in, from the highest discount percentage to the lowest, then they apply to the consolidated usage
            across all  other accounts, from the highest discount percentage to the lowest.
            
            It is recommended to purchase all Savings Plans in an account with no usage or resources, such as the master account. This ensures that the Savings Plan applies to
            the highest discount rates across all of your usage, maximizing the discount amount
            
            Frequently review the Savings Plans recommendations in Cost Explorer (perform regular analysis)          

                          
            • Third-party agreements and pricing
           
Plan for Data Transfer
             Networking resources in the cloud are consumed and paid for in the same way you pay for CPU and storage—you only pay for what you use.
            > Perform data transfer modeling:Model your data transfer with a sample (eg: data sync between AZ or region) and calcluate the cost. Use AWS cost explorer tool or cost and usage reporting tool with the model to get an idea.
            
            > Optimize Data Transfer : Consider using CDN or AWS Direct connect / VPN depending on Workload / VPC End point( Allow connectivity between AWS Services to reduce public data transfer)
            
            
            
            Manage Demand and Supply Resources
             Manage Demand – Throttling -  
              If the source of the demand has retry capability, then you can implement throttling.Amazon 
              API Gateway can be used to implement throttling
              
             Manage Demand – 
               Buffer based - Similar to throttling, a buffer defers request processing, allowing applications that run at different rates to communicate effectively. 
               Amazon SQS is a managed service that provides queues that allow a single consumer to read individual messages
               Amazon Kinesis provides a stream that allows many consumers to read the same messages
               
            Dynamic Supply
             Use Autoscaling
             Use metrics and events( CPU, Network etc) from cloudwatch to trigger scale events in your workload
             Elastic Load Balancing (ELB) helps you to scale by distributing demand across multiple resources
             
            Time-based supply: 
             A time-based approach aligns resource capacity to demand that is predictable or well-defined by time.
             You can also leverage the AWS APIs and SDKs and AWS CloudFormation to automatically provision and decommission entire environments as you need them
             This technique can also be applied to other resources, such as EBS Elastic Volumes, which can be modified to increase size, adjus performance (IOPS) or change the volume type while in use
             When architecting with a time-based approach keep in mind two key considerations. First, how consistent is the usage pattern? Second, what is the impact if the pattern changes?
            
            Develop a workload review process:
            To ensure that you always have the most cost efficient workload, you must regularly review the workload to know if there are opportunities to implement new services, features, and components
            
            
            
            
            
  AWS Services:
  
  Security: 
  IAM
    Centralized control of the AWS Account
    Shared access to Accounts
    Users -> added -> groups - > given -> Permissions
    power user - All service access except manging IAM groups and users

   Same account resources/  Users in other account  / Web Identiy / SAML -> Roles -> add -> Permissions
   ( The concept of Roles seems to be different from GCP. See https://www.stratoscale.com/blog/compute/comparing-google-iam-aws-iam/. In GCP Users and Groups are identity and a policy is a binding Identity with roles. The Roles are a group of permissions.)

    How can you give permission to another account in AWS and GCP?

    In AWS the role is answer, IN GCP, the user can be added with email address as an identity. In AWS the, external user needs to have AWS account. - need to revisit?

   Monitoring:
    CloudWatch : Used for collecting logs, metrics, events , creating alarms, creating dashboards for monitoring performances. It is integrated with various notification channels for alarms. By default cloud watch monitors EC2 performance for 
    every 5 minutes. With Detailed level ( configured in Ec2), it can monitor every 1 minute.
    System Manager : It is collection of service and capability for configuring and managing Ec2 instances, On primese servers and Virtual machines and certain other resources. 
    
    Stoarge:
    
     Object Store : S3 ( Simple storage Service) 
      S3
       Buckets
        Objects
        
      Bucket Features:
       Storage Type
       Lifecycle Management
       Versioning ( once enabled, you can be only suspended. Applicable only for new writes)
       Encryption
       Bucket Policies
       MFA Delete ( Protect Object)
       Cross region replication ( Versioning needs to be enabled, existing files and delete markers will not be replicated.)
       Transfer Acceleration( Using cloudfront to upload object by users and making use of AWS network)
      
      Object Structure:
             Key
       ACL   Value       VersionID
             Meta-data
             torrents      
      
      Consistancy:
       Read after write policy
       Eventual consistancy for Put and delete
       
     Durability 99.119%
      Latency : Milliseconds -> 12 hours depnding on storage class
      
      S3 Tier:
       Standard - 99.99% availability
       S3 Infrequent Acceess ( 1 Month Minimum with retrieval fee) -99.9% availability
       S3 Intelligent Tier (1 Month Minimum) -99.9% availability
       S3 Infrequent Access 1AZ ( 99.5 availability,1 Month Minimum with retrieval fee))      
       S3 Glacier ( configurable Minutes to hours retreival time ,  3  months min with retrieval fee)), 99.99% availability
       S3 Glacier Deep Archieve ( 12 hour retrieval time,  6 Months min with retrieval fee)), 99.99% availability
       
      
   Points to note :
    Not suetable for operating system. You should use block stoarge for that purpose.
    File can be from 0 bytes to 5TB. Largest single Put can transfer upto 5 GB after that use multipart.
    Http 200 when you upload a file to S3.
    More prefix, more performance. 5500 get/head/s request, 3500/s other operations per prefix. The upload and retrieval will count towards KMS quota. Keep this in mid while designing.
    This is a regional resource. Users can select the region.
    Virtual style puts your bucket name 1st, s3 2nd, and the region 3rd. Path style puts s3 1st and your bucket as a sub domain. Legacy Global endpoint has no region. S3 static hosting can be your own domain or your bucket name 1st, s3-website 2nd, followed by the region. AWS are in the process of phasing out Path style, and support for Legacy Global Endpoint format is limited and discouraged. However it is still useful to be able to recognize them should they show up in logs. https://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html
    100 S3 bucket per account?
    Security and complience:
     Access
      Customers may use four mechanisms for controlling access to Amazon S3 resources: Identity and Access Management (IAM) policies,
      bucket policies, Access Control Lists (ACLs), and Query String Authentication (With Query String Authentication, customers can create a URL to an Amazon S3 object which is only valid for a limited tim)
      Amazon VPC Endpoint for Amazon S3 is a logical entity within a VPC that allows connectivity only to S3. You can configure the specific VPC end point using AWS S3 bucket policies.
     Encryption:(SSE-Server side encryption)
      SSE-S3 -Use default S3 key to encrypt and decrypt. Managed by AWS
      SSE-C - Use customer supplied key to encrypt and decrypt. Customer manages and supplys keys to AWS during opeartion
      SSE-KMS - Use Key managed by Customer in AWS KMS. 
      Client Library - Client encrypts the object before storing. Fully owned by client. AWS S3 encryption client can be used.
     Audit:
      S3 object access can log into cloud trail
      Access Analyzer for S3 alerts you when you have a bucket that is configured to allow access to anyone on the internet or that is shared with other AWS accounts
     GDPR:
      Choose your EU region
      Choose AWS S3 on Outpost(On premise S3 solution managed by AWS)
     PII
      Amazon Macie - Uses AI to lookn for PII
     S3 Object Lock:
       Write once read only mode ( lock the object)
       Complience mode - No one can delete/ modify it
       Governance mode - Only specific users can unlock it and modify or change lock.
     S3 Glacier Lock
       Specify a policy and lock the object. Once locked the policy cannot be changed.
      
     Services around S3
     
     Query and Analyse:
          S3 Byte range fetches : To fetch first few Bytes.
          Amazon S3 Select: Retreive part of an object using SQL query language to a dataset . Saves data transfer if you know what to query.        
          Amazon Athena : Analalyse objects using Query service on top of S3 using Sql           
          Amazon Redshift: enables you to run queries against exabytes of unstructured data in Amazon S3 with no loading or ETL required 
          Amazon Macie : To identify PII data
          S3 Access analyser : Analyse the access to S3 Bucket
         
          
     Notification:
       S3 Event notification : Sents events to SNS, SQS, Lambda for S3 Http methods
       
      Transfer Objects to S3:
           Using S3 standard https url            
           Amazon CloudFront’s PUT/POST : 
            Consider for Data < 1 GB
           S3 Transfer Acceleration :
                 to Upload S3 object Quickly using cloudfront. Once enabled, it will give transfer acceleration end point which can be used by client to put objects
                access restriction based on a client’s IP address, are supported as well
                GB-> TB
                AWS has expanded its HIPAA compliance program to include Amazon S3 Transfer Acceleration as a HIPAA eligible service
          AWS Snow Family: 
            Use it for large batch one time transfers. Usual turn around time is 5-7 days.Consider this timeframe when designing solution by comparing the network speed.
           Snow ball
           Snow ball Edge
           Snow Mobile
           DataSync
            Used as a datasyncup solution between on prem and S3, EFS, Amazon Fsx for windows. It can be also used to sync data between two EFS. 
           AWS Storage Gateway
            AWS Storage Gateway is a physical or virtual appliance that can be used to be a gateway ( not sync) for your data between on prem and S3. It can use ( complement) S3 transfer acceeleration for efficiency.
            Variation
              File Storage Gateway - Files are stored in S3 and storage gateway act as NFS or SMB to application.
              Volume storage gateway - Used as a backup of block storage to S3. The gateway keeps backup of your volume snapshot into S3.
              Cached Vloume - Here volume is entirely stored in S3 only frequently accessed part is kep on premise.
              Tpe gateway - This can be used where backup is stored in tapes now. Instead of tape, replace it with tape gateway and the backup will go to S3.
               
       How to Share S3 between accounts?
          1. Using Bucket policies and IAM
          2. Using Object ACL and IAM. This is only programatic
          3. Using cross account IAM roles.
           
        
            
EC2

 Ec2 instances pricing types
  On Demand
  Reserved capacity ( instance/compute)
   Standard reserved instance : reserve an instance (72% discount)
   Convertable reserved instances : Can convert between instance type (54%)
   Scheduled reserved instances: Available to launch during a scheduled time frame.
  Spot  ( like pre-emptiable)
   If amazone terminates it, you will not be charged for the partial hour otherwise you will be.
  Dedicated host - can be complemented with reserved capacity
   Used when complience or licensing requirments are there.
  
  Ec2 instances machine Type:
  
    Fight DR Mc pxz AU
    F - Field array
    i - Io optimized
    g - Graphics optimized
    h - High disk through put
    t - tiny general
    
    D - Dense storage
    R - Ram optimized
    
    M -Medium general 
    P - Picture, general purpose GPU, machine learning
    x - Xtream memory optimized used for big Data
    z -High compute and Mem
    
    A - arm based
    U - Bare metal
    
   Ec2 notable features :
    Termination protection - Have to explicitly select it
    Root Volume  - Default is delete when terminated. If needed can be changed.
    Additional Volume - Default is not to delete when terminated. Can be changed.
    Hibernation - Ec2 can be asked to hibernate its RAM state. This is good if we have more time in starting up the instance or long runnig process. 
    The RAM must be  < 150 GB and is available only for selected instance type. Maximum period of hibernation is 60 days.
   Automation    
    User Data : This field can be used to run autmated scripts when instance starts. The scripts are called bootstrap scripts.
    Instance metadata : Ec2 instance can access instance metadata server to get the user data or meta data of the server. The metadata server url is https://169.254.169.254/latest/user-data or meta-data
    Monitoring
     System status checks -checks the underling hyperware health status
     Instance status checks - checks the instance health status
     Cloud watch monitoring
    Encrytion :
     Root volume can be encrypted when created.
    Security
     Security Group - Security group by default block incoming and allow outgoing. You cannot put a deny rule in security group for that you have to use network ACL.
                      Instead, security group deny all incoming and you can override it by using allow rule. Security groups are Statful firewall to instance. 
                      You can have m->m relation ship between ec2 and security group.
     Roles : Prefer roles instead of storing credentials. Roles are generic concepts for AWS resources to access another resources or services. As a best practise, use role than storing credentials in Ec2 ( by using aws configure) to 
              access other resources.
    
    Ec2 placement Group:
     Way of placing the Ec2 instances
     You can't merge placement group
     you can't move existing instances into placement group after stopping.
     Types:
       Cluster – packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications.

       Partition – spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka.

       Spread – strictly places a small group of instances across distinct underlying hardware to reduce correlated failures.
                      
     EBS Volume:
      Aws replicates EBS volume within AZ to ensure high availlability and durability.
      Types :
       General pupropse SSD ( Most worklods), gp2
       Provisioned IOPS SSD ( Database),io1
       Throughput optimized HDD ( BigData and Datawarehouses),st
       Cold HDD(File Servers),sc
       EBS Magnetic- previous gen HDD ( Infrequently acceesed Data),standard
      Hardwire Assisted virtual machine image type will give lot more flexibility to select different types of VM
      When we create a snapshot, the snapshot will get store in S3. Snapshots are point in time and is incremental.
      Images are full and is not incremental
      Duplicate a EC2 instances in another / Same AZ
      
       Disk -> Snapshot -> Image -> EC2
      
      Duplicate  a EC2 instances in sanother region
      
       Disk -> Snapshot -> Image -> Copy Image to another region -> Ec2
       
       How to encrypt an unecrypted volume?
       
       Un encrypted Disk -> Snapshot( encrypted) -> Image -> Ec2
        Note: you can only share a snapshot between accounts only if it is unecnrypted.
       
      AMI is a regional resoruces.
      Snapshot can be shared from one account to another as well with appropriate IAM permissions
      You can take a snapshot while instance is running but best practice is to stop the instance to take snapshot
      volumes will always on same availabllity zone for better performance
      Volumes exists in EBS. Volume is a logical compenent/connection which is tied to an EC2 instance and reside in EBS. EBS is simply the harware part of the volume which is again a virtual hard disk.
      
      
     AMI - Amazone machine image
      Types : 
        EBS
        Instance store - This storage is physically located to a disk which is attached to machine where VM is created.
                         When a VM is stopped / terminated , this storage is reset as AWS may change the physical host/ hyperwiser when a VM is restarted, terminated .
                         So the storage is ephimeral. This can be used for buffers, cache data and should not be used for persistant data. 
                         Since the disk is attached to physical hardware, the instance store cannot be added after creating the VM and is not available with all instance type.
        
      Networking on EC2 instances:
      
       EC2 uses Network inteface to communicate to outside of it's machine. The network interface is a logical component of VPC and can be attached to EC2 instances. 
       The network interface inheritis the IP address allocation of a subnet when created. If later point in time, the address is changed, it does not reflect in the 
       Network interface. When an Ec2 is created, there is an automatic primary network interface created and attached to it which allocates the public and private IP address
       from its ip address configuration.
       
       The primary network interface cannot be detached or removed. You can attach multiple secondary ENI to the Ec2 instance. Mainly the use case are like, you want to 
       attach same machine to multiple subnet( say with private and public subnet group) or when creating security applience in the VPC.
       
       If a secondary network interface is detached and attached to a new instance, all traffic via secondary network interface will flow to the new instance. Hence this solution 
       also can be used for a low cost load balancer scenario.
       
       If you need enhanced networking, the you have to use either ENA ( enahnced network adapter 100 GPS) or Intel VF( virtual function 10 GPS) interface. Enahnced network allows the 
       more throughput and bandwidth for the network communications.
       
       When using an ENA, there is a OS layer for TCP IP communication. For application that need high performance / machine learning application, there is an optimized solution
       to use EFA ( Elastic fabric Adapter)  which bypassed Application -> EFA by using another intermediate module called Libfabric ( it uses a "fabric" solution) which is better 
       performant than tcp ip module in OS.
     
   File System
   Unix/Linux: Elastic File System (EFS) :
   
     EFS allows multiple EC2 instance to access share file volume. 
     Size is eleastic and changes with file size, You don't have to choose the size. The size can scale upto petabytes
     EFS also have life cycle management
     throughput type - Bursting < Provisioned
     Performance - General < Max I/0
     Encryption at rest can be choosen by user when provisioning
     Once created , you will have to mount the EFS into Ec2 instances by running commands supplied by AWS and can be obtained from EFS section of AWS console.
     Support NFSv4 protocol
     cost is based on storage
     Support 1000 concurrent NFS connnections
     Data is replicated to multiple AZ within a region for durability and availablity.
     Read after write consistancy model
     
   FSX for Lustor:
     Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance storage for compute workloads. 
     Many workloads such as machine learning, high performance computing (HPC), video rendering, and financial simulations depend on compute
     instances accessing the same set of data through high-performance shared storage. This can be integrated with S3 to store data in s3..
     
   Windows : FSX for windows file server
     Designed to use with microsoft applications - SQL server, IIS, Active directory, Share point
     SMB( Windows Server message Block) based
     

     HPC Industies:
      Genomics
      Finanacial
      Machine learning
      Weather protection
      Autonomous Driving
      
       HPC Types:
        Data transfer :  Use snowball / Snow mobile (TB/PB of data)
                         AWS Data Sync
                         AWS Direct connect
        Compute :
                      : Ec2 instances GPU or CPU optimized
                      : Placement group
                      : Ec2 Fleet
        Networking : 
                      Enhanced Networking
                         ENA
                         EFA
         Storage :
               
                      Instance attached Storage
                         EBS with provisioned IOPS
                         Instance store
                      Network Storage:
                         S3
                         EFS
                         FSX for Lustor
         Orchestration and Automation:
                      AWS Batch
                      AWS Parallel cluster
         
      Security :
      
       AWS WAF - Layer 7 Application layer Http/Https firewall for cloud front, load balancer, API gateway and instance group. Allow/Block or clount.
       Network ACL
       Security Group
       
       AD services:
       
       Generally, Active directory is based on LDAP and DNS services. It is a windows solution used to manage the access to differents services and machines with entitlement, group and roles. 
       It is similar to IAM but is a microsoft solution. It supports authentications like Kerboros, LDAP and NTLM(?) authentication. The authentication modules can be integrated with services. 
       User can use their ERS account to authenticate and authorize with different services based on the entitlements/groups/role in AD. Typical AD requires HA.
       
       AWS has several family of AD services.
       
       AWS Directory Services 
        This is a managed AWS services PAAS which can do the following
         1. Setup a AD service in AWS ( AWS managed AD)
         2. Connect to on prem AD service and control the AWS microsoft AD aware services and Ec2( windows and linux) machine management through existing configuration in the on prem AD service
         3. Use existing co-orporate credentials to access AWS resources using IAM policies configured for the co-orporate credentials. This is federated access. The authentication 
            will be managed by on prem / AWS AD service and authorization will be managed by IAM.
         4. SSO can be implemented in Ec2 instances. There is no different credentials needed for Ec2 . You can use SSO with co-orporate credentials.
         

        Authentication and Authorization for employees:
        
          AD Trust:
           Used to Join on prem AD service to extend the services. This means the same AD configurations can control/access both on prem and aws applicable services or resources..      

          AD CONNECTOR:
           This is a pure proxy( directroy gateway) service to on prem AD service.No footprint of Ad configuration in AWS. Functionalities are similar to AD trust.

           AWS Managed AD
           used for Seting up a AD service in AWS ( AWS managed AD)
           AWS run the AD services in its own VPC. Your services will be able to access the AD services VPC.
           Can be integrated with AD Trust

          Simple AD:
           Similar to AWS managed AD with less scale and basic features.
           It can support only 500-5000 users depending on small or large
           Cannot be integrated with AD Trust.
           
         Directory Service solutions for Development
          Cloud Directory:
           Managed Directory solution for applications which neeeds to make use of a directory solutions like Org charts, product, course catelog etc. 
           This is not a Active directory ( microsoft solution) rather it is just a directory service where users can write their on logic based on directory.
           
         Federated Authentication Services:
          Amazon Cognito user pools:
           This is used for application sign in for web and mobile.
           The user credentials can be managed in Cognito or it can federate the access to another service provider iike google or facebook
         
      IAM:
      Policies:
       Policies contain JSON document with the following building blocks
        ARN 
         : Arn are used to identify a resource ( resource name) with syntax arn:partition:service:region:account_id
         : eg : arn:aws::s3:::mybucket/image
         
      Json Syntax :
       Version
        - Statement
           Sid (Statement ID)
           Effect( Allow/Deny)
           -Action( API call)
            API:method (eg : dynamoDb:Scan)
           Resource: ARN
           
         
         
      Usage:
       Policies can be attached to User , Group and roles and is the known as identity policies
       Policies that are attached to resources are known as resource policies.
       Ideally for identity , A group should be created with required policies and users should be added to group.
               For resources, a role should be created with required policies and resources should be given the role.
       
       Types of policies:
        AWS managed policies
        Customer managed policies
        inline policies ( applicable only for the Identity or resources when created. Others will no have visibility to this policy).
        
       Policy eveluation:
        All policies are combined first to eveluate.
        Default is deny unless aloow is specified
        Any deny on same arn will override allow.
        AWS will take the policy and then apply permission boundary before allowin the access to resource.
        Permission boundaries controls the maximum permission allowed for an user.
        
    Multi Account scenarios: 
        
      AWS Resource manager:
       AWS resource manager is used to share resources between aws account.
       One account can choose to share the resources from its aws resource manager and add other accounts account id to slected resource to share. This will trigger an
       Invitation and other account have to accept it. Once done, the resource is shared between two accounts.
       - Not every resource is available for resource sharing though aws resource anager
       
      AWS Single sign on:
        AWS SSO is a single sign on service which can intgrate with different services like, AWS AD, AWS Trust, Business applications ( like Gsuite, Oracle etc) and 
        can be used to manage access for  multiple AWS accounts under a organization. This can also be integrated witb SAML 2.0 supported applications.
        
    DNS services
     Domain register: This is a service which will allow you to register a domain. It can take 3 days.
    
     Route53 - DNS is on port 53. Thus name.
          DNS 101: 
           Domain -> IPV4 or IPV6
           IPV4 - 32 bits, 4 octet
           IPV6 - 128 bits 
           Tope level domain - .com
           Second level domain name -.co in .co.in
           Domains are given by IANA ( internet assigned number authority)
           Domain registar - can assign domain names. This is integrated with ICANN
           ICANN - Enforces the uniquenes of domain name acress internet by registering whois database
           SOA - Start of authority
           NS - Name server records
           inspire.com->will first go to .com( top level domain server) - > Get the NS record - > ns.awsdns.com( domain registers name server) - > The name server will have SOA record
           Inside SOA -> DNS records
           SOA - will also have TTL and info about other DNS records. Default ttl is 48 hours. The webserver will cache the TTL.
           Naked domain name : inspire.com
           Sub Domain: www.inspire.com, m.inspire.com etc

           DNS Records :

            NS record : refer above
            SOA record: refer above
            A record : Address record. Maps Name -> IP address4
            AAA records : A record for ipv6
            C Name : Canonical name. Used to map a sub domain to another domain or subdomain which has an A record or alias. For ex: m.inspire.com->inspire.com
            Alias record : Used to map naked domain to another domain. eg: inspire.com -> s3 bucket, ELB etc
            MX record : mail server exchange records
            PTR record : point of reference record. It is reverse of A record.
            TXT record : any text that would be added to DNS record
            SRV record: ?
         
         Routing policies:
         
           Simple routing: 
            A records can have multiple IP address in seperate lines. AWS rnadomly chooses one to route the traffic to. 
           Weighted routing:
            Split the traffic based on different weights. Say 10% to one IP and 90% to another.
           Latency based routing:
            AWS chooses the least latensy with respect to user to route the traffic to configured backed service
           Failover routing:
            Used for active passive setup. A DR scenario. If primary fails, the traffic will be routed to secondary.
           Geolocation based routing:
            Used to route the traffic from a geo location to a purticular resource in AWS. Consider this for a GDPR scenario.Locatin can be based on continent or country.
           GeoProximity based routing:
            Allowed only when trafic flow rules are enabled. In the traffic flow rule, you can configure geoproximity location routing rule by specifying a lattitude and longitude and a bias
            to rote the traffic to another rule or to server. You can combine multiple rules in traffic rules.
           MultiValue answer routing:
            Simple routing + health checks
           
         Health checks and Alarms:
          Route 53 can check the healthchecks before routing to a IP address with any of the above configured policies except Simple routing.
          AWS does not route the traffic until it pass the health checks.
          
          Limits:
          Soft limit of 50 domains
          
        Networking 101
        Private IP address
         10.0.0.0-10.255.255.255 (10/8)
         172.16.0.0-172.31.255.255.255(172.16/12)
         192.168.0.0-192.168.255.255(192.168/16)
         BGP - a protocol for transferring dynamic routes between routers
        
        subnet creation 
         cider.xyz
        
        AWS VPC:
        
         AWS largest subnet that can be used : 10.0.0.0/16 and smallest would be /24
         5 ip address from host pool in each subnet is reserved by aws
         
         
         Subnet is AZonal resource
         VPC Components :
          VPC
           VPC are regional in aws
            Subnet        
                Private
                Public :                 
                 In order to make it public, the ec2 instances in subnet also should be assigned with public ip address. This can be done through auto assign public ipv4 address.
                 create an Internet gateway and associate with VPC. 
                 Create a route table and associates with subnet. Define routes for both ipv4 and ipv6 to route to IGW for any internet destination address.
                 Create a security group and associate it with subnet. Allow http/s/ssh traffic from source internet to corresponding port
                 
           External connections  interfaces     
                 VPC <-> Internet
                 
                           Internet Gateway ( only one per VPC)
                   
                   VPC -> Internet

                           NAT Gateway
                                  This for private ec2 instances to communicate to internet.
                                  Have to be launched/associated in public subnet which has connection to internet gateway
                                   In the private subnet, create a route from private subnet routes to natgateway or nat instance
                                   NAT Instances : 
                                    Indvidual NAT instance. This is not HA by default. You will have to create and manage your on HA architecture if using instance.
                                    Create EC2 instance with NAT AMI
                                    Disable source and destination checks as NAT will communicate to internet on behalf of other severs.                   
                                   NAT Gateway : 
                                    HA Gateway managed by AWS 
                                    Create an elastic ip for natgatway whle creating
                                    Limited to one AZ. For HA architect, create NA in each availability zone and attach to subnets that are in the AZ.
                                    
                    VPC -> AWS end point ( through AWS network)
                                VPN End points ( refer below)
                                 Network interface
                                 VPN Gateway
                    VPC -> VPC ( Through AWS)
                              Private Link
                    
                    VPC -> VPC -> On Prem                    
                             AWS Transfer Gateway
                                 This is a gatway acting as a HUB (AWS transfer gateway) and Spoke model to different VPN, AWS Direct connection, and VPC to VPC connection.
                                 The connection routes can be controlled by configuring it in AWS transfer gateway
                                 This makes network architecture simple
                                 This is an ideal solution where transitive peering is needed between multiple vpcs and/or on prem datacenter
                                 Supports IP multicast. There is no other service that supports this feature in aws.
                     Multiple VPN Connection
                              VPN CloudHub - it is a VPN Hub with a hub and spoke model                            

                  Internet/on prem -> VPC
                         Bastion hosts:
                            Jump boxes to connect to your private subnets. Opposit of NATInstances.
                
                Network layer connections between on prem and vpc:
                   
                         VPC - internet - On Prem
                          Virtual private gateway ( VPN)
                           VPN Gateway
                           Customer Gateway

                         VPC - Partner /customer ( unencrypted)
                          Direct connect

                          VPC - Partner /customer ( encrypted)                  
                          VPN over Direct connect                                   

                 Network layer connection between VPC inside

                       VPC - VPC
                         VPC Peering
                         Shared VPC??? HOw to do that?
                         VPC-IGW-VPC ( traffic leaves internet)
                         PrivateLink
                          Using private link, you can share your vpc to another vpc without having the traffic leaving internet. This is 
                          good solution if your service in a vpc needs to be shared with another 10 or 100 of other customers.
                          Private link is essentially consist of a load balancer with Elastic IP at your side and an ENI ( elastic network interface)
                          at customer side. The ENI needs to be attached to customer's ec2 instance and a private link needs to be configured to connect ENI with ELB.

                       VPC - supported AWS services through AWS network
                        VPC endpoint
                         No bandwidth constraint
                         Traffic does not leave internet
                         This is powered by private link of aws infra.
                          Type of interfaces :
                           Interface endpoints
                            - This is an Elastic network interface that will be attached to EC2 instances to communicate to AWS supported APIs privaty. 
                           Gateway endpoints
                            - Gateway is like a NAT Gateway for connecting to AWS supported API from aprivate subnet. When creating, you need to attach to a vpc and subnet.
                              routes will be automatically created from subnet to this gateway to commmunicate to aws apis.
                         
           
                    
           Routing (  VPC resource but can be attached to subnet)
            Route Tables
             The main route table are created with VPC and is automatically assigned to all subnets in VPC.
             For public subnet, create another rote table as the main route tables are associated with all subnet
            Sharing
            ...
           Security
           
            Network ACL         
             Network  ACL are stateless firewall rules 
             A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.            
             Can configure allow and deny rules
             The highest number that you can use for a rule is 32766 ( lowest prirotiy)
             Have to associate with a subnet else subnet will get associated with default NACL
             1 subnet can have only 1 NACL but NACL can be reused across multiple subnet
             Ephimeral ports 
              : When a machine gets a request in one of the ports, say 80, the outbound message will go through ephimeral ports.
              : So you have to make sure that ephimeral ports are allowed from the target depdending up on communication scenario.
              : For example if you are acting as a server, then you need to allow inbound from client to your http/s and outbound from your servers ephimeral ports to outside. Vice versa.
              : Ephimeral ports: - 1024-65535
              Custom :
               You can create a custom network ACL and associate it with a subnet. By default, each custom network ACL denies all inbound and outbound traffic until you add rules. 
              default:
              Your VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic
             
            Security Groups 
             Inbound and outbound rules can be configured
             Security groups are staeful
             Can configure only allow rules
             
            Load Balancers:
             Load balancers can be created only if we select atleast two public subnets ( AZ). This may be due to have HA for loadbalncer machines.
             504 - Gateway time out. Meaning your back end servers are busy. This is not an issue with load balancers but with backend.
             x-forwarded-for : get the ipv4 client address
             There is no IP address given for load balancers. It is only DNS name. This is where your alias record comes into play in DNS configuration for
             Naked domain and C-NAME for sub domains.
             You can configure health checks to your backend servers.
             You can enable cross AZ load balancing for HA of the load balancing. Even if some of the backed servers in one AZ goes un healthy, the load balancer in that AZ can 
             distribute the traffic to other instances in other AZ.
             Loadbalancers can be put into their on security group.
             You can enable or disable sticky sessions in load balancer
             Types:
                    Application load balancers
                     Application aware load balancing 
                     load balancing rules can be configured based on header or path
                     Layer 7 load balancers
                      HTTP
                      HTTPS
                    Network Load Balancers
                     Layer 4 load balancers
                     Used for high performance
                      TCP
                      UDP
                    Classic load balancers
                    Round robin load balancing only
                    Basic layer 7 features like X-forwarded for , Sticky session. This is not url or application aware and does not operate in layer 7
                    No intelignet routing as application load balancers.
                    Monitoring can be done using cloud watch
                    Cost effective compared to application load balancing
                     HTTP
                     HTTPS
                     TCP
                 Traffic:
                  Internal
                  Internet-facing
                TargetGroup:
                 Target groups are groups of backend servers where your load balancers needed to distribute the traffic into. You can have say one target group for UK and other for US
                 and configure your load balancers to distribute the traffic to specific target group based on load balancing rules
                  TargetType:
                   Instance
                   IP
                   Lambda
                

             

          
           Logging:
            VPC Flow logs 
             Can capture network traffic for different level and send to cloudwatch or s3 buckets
              VPC level
              Subnet level
              Instance level
             You can only enable flow logs for VPC which are in your account. You cannot enable for another vpc which is peered but resides in another account.
             Once created, you cannot change the configurations.
             Any internal aws specific ( metadata, DHCP, DNS) etc are not monitored.
        
        Default VPC:
         All subnet in default vpc have access to internet
         All Ec2 instance can get a public and private ip address.
         
         VPC - VPC connection: 
          VPC peering ( Same account or others. No transitive peering)
          Cross region VPC peering is allowed
       
       Increasing performance
        Client -> website
         Gllobal accelerator
          Client -> edge location -> global accelerator ->through aws network -> end points ( Load balancer, Ec2, Elastic ip)
          Optional multiple regional end points
          global accelerator will give two ip address for high availability
          Traffic can be controled with different policies like weighted
        
       Increasing Availability
      Elastic load balancer:
       ELB does not have a IP address. You will have to map your DNS to ELB using CNAME or Alias record depending on wether it is subdomina or naked doamin.
        
      Network cost
       Incoming traffic to a VPC is free
       Communication between rsources within a AZ using private ip address is free
       Communication between resources using pulic ip address is charged as it leaves to internet
       Communication between two AZ is chargable
       Communication between two regional is chargable and will incure additional regional communication charge as well.
       
     AutoScaling:
     Click on launch configurations to create the template before creating autoscaling group
     Warm up time period can be configured to control the startup time
     Activity time history will show the information about instance start and stop
      Components:
       Groups ( Application server group , database server groups, webserver groups etc)
       Configuration templates ( instructions)
       Scaling options
        Maintian specific instances # all times
        Manual scaling ( Change max, min manually)
        Schedule scaling
        On Demand Scaling ( reactive)
         eg: scale 50% of the load is met
        Predictive Scaling ( proactive)
         intelligent scaling
        combine predictive and on demand scaling to get optimal performance and availability
  
 HA Archicture:
  Plan for failure
  Netflix simian Army projects for chaos testing ( injecting failures)
  Use multi AZ and Multi region wherever possible
  Scale out instead of scaling up ( performance) wherever possible
  Remmber read replicas for performance and not for HA
  S3 single AZ is not highly available
  eg:
  user -> route 53 - load balancer 1 ( region 1) - public subnet ( webserver autoscaling group) - internal load balancer (app server autoscaling group) - Datbase
                                                  
                   - load balancer 2( region 2) 
  Configure health checks
  HA For Bastion host:
   Use Network load balancing to load balance the Baston host( different AZ) for HA and configre SSH or RDP (TCP). This is costly as network load balancing is involevd.
   For a cost frontly sloution, use auto sacling group with EIP for a bastion host. If it fails, the autoscalng group will create another machine in another subnet and use an 
   automated script to get the new EIP for new instance. You might have some down time in this scenario.
  
  
  Cloudfront
   : distributions:
     Web
     RTMP( for media streaming)
     
  RDS failover:
   Reeboot with fail over will force the RDS to launch in a different availability zone.
   
   Cloudformation;
    Sripting your architecture and automate the provisioning of AWS resources
    
    Elastic bean stack
     This is under compute.
     This is similar to Google APP engine. A PASS modeal for deploying your code/application and edit your configuration ( capacity, load balancing, security, health status etc).
     
        
    On Premise Solutions from AWS
        
       
        
        
       
       
       
       
     
       
      
      
           
           
         
        
         

         
         
         
         
            
                            
                      
                         
                         
                         
                         
                         
                         
        
        
      
   
       
       
       
       
       
     
    
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
